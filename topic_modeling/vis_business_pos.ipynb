{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#df = pd.read_pickle('./dataset/State_Hotel_reviews_v1.pkl')\n",
    "#df = pd.read_csv('./State_Hotel_reviews_v1_pred.txt', encoding='utf-8', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'How many records: {len(df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Distribution of [Overall Rating]:\\n{df[\"Overall Rating\"].value_counts(normalize=True)}')\n",
    "print(f'Distribution of perspective [Value]:\\n{df[\"Value\"].value_counts(normalize=True)}')\n",
    "print(f'Distribution of perspective [Location]:\\n{df[\"Location\"].value_counts(normalize=True)}')\n",
    "print(f'Distribution of perspective [Rooms]:\\n{df[\"Rooms\"].value_counts(normalize=True)}')\n",
    "print(f'Distribution of perspective [Service]:\\n{df[\"Service\"].value_counts(normalize=True)}')\n",
    "print(f'Distribution of perspective [Sleep Quality]:\\n{df[\"Sleep Quality\"].value_counts(normalize=True)}')\n",
    "print(f'Distribution of perspective [Cleanliness]:\\n{df[\"Cleanliness\"].value_counts(normalize=True)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemmatize_tokens(text, wo_stopword=True):\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import TweetTokenizer\n",
    "    import string\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    import re\n",
    "\n",
    "    text = \" \".join([re.sub(r'^h?ttps?:\\/\\/.*[\\r\\n]*', '', item, flags=re.MULTILINE) for item\n",
    "                     in text.split()])\n",
    "                     #in text.replace('\\n', ' ').strip().replace('\\xa0', '').split()])\n",
    "\n",
    "    text = \" \".join([re.sub(r'^doi:.*[\\r\\n]*', '', item, flags=re.MULTILINE) for item\n",
    "                     in text.split()])\n",
    "\n",
    "    text = \" \".join([re.sub(r'^www.*[\\r\\n]*', '', item, flags=re.MULTILINE) for item\n",
    "                     in text.split()])\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    string_punctuation=list(f'{string.punctuation}:”“….....’–‘—•')\n",
    "    #with open(\"./dataset/stopwords_manual.csv\", \"r\", encoding='utf-8') as f:\n",
    "    #    stopwords_manual = [item for item in f.read().split(\"\\n\") if item]\n",
    "    sp = set(string_punctuation +\n",
    "             stopwords.words('english') +\n",
    "             #stopwords_manual +\n",
    "             ['\\xa0', '\\xad', 'et al', '\\uf07c', '\\x00'])\n",
    "\n",
    "    tokens = [token for token in nltk.TweetTokenizer().tokenize(text.strip().lower())]\n",
    "\n",
    "    if wo_stopword:\n",
    "        tokens = [token for token in tokens if token not in sp\n",
    "                  #and token not in stopwords_manual\n",
    "                 ]\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df[\"text\"] = df[\"Review Title\"].astype(str) + ' ' + df[\"Review Content\"].astype(str)\n",
    "#df[\"token\"] = df['text'].replace(\"\\xa0\", \" \").replace(\"\\n\", \" \")\n",
    "df[\"token\"] = df['text'].map(get_lemmatize_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Travel Type Label\"] = df[\"Travel Type\"].map({\"Traveled on business\":\"Business/Solo\",\n",
    "                                                 \"business\":\"Business/Solo\",\n",
    "                                                 \"Traveled solo\":\"Business/Solo\",\n",
    "                                                 \"Traveled as a couple\":\"Friend/Couple\",\n",
    "                                                 \"friends\":\"Friend/Couple\",\n",
    "                                                 \"Traveled with friends\":\"Friend/Couple\",\n",
    "                                                 \"Traveled with family\":\"Family\",\n",
    "                                                 \"family\":\"Family\",\n",
    "                                                })\n",
    "\n",
    "df[\"Travel Type Label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset Pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_pickle('./dataset/State_Hotel_reviews_v1_240415.01.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version='240418.01'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business/Solo Positive (4 or 5 顆星)\n",
    "\n",
    "Location, Service, Sleep Quality, Cleanliness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Location, Service, Sleep Quality, Cleanliness\n",
    "travel_type = \"Business/Solo\"#\"Friend/Couple\", \"Family\", \"Business/Solo\"\n",
    "df_business_pos = df[df['Travel Type Label'].str.contains(travel_type, na=False, regex=False)]\n",
    "df_business_pos = df_business_pos [#df_business_pos [\"Overall Rating\"].isin([4, 5]) &\n",
    "           #df_business_pos [\"Value\"].isin([4, 5]) &\n",
    "           df_business_pos [\"Location\"].isin([4, 5]) &\n",
    "           #df_business_pos [\"Rooms\"].isin([4, 5]) &\n",
    "           df_business_pos [\"Service\"].isin([4, 5]) &\n",
    "           df_business_pos [\"Sleep Quality\"].isin([4, 5]) &\n",
    "           df_business_pos [\"Cleanliness\"].isin([4, 5])]\n",
    "\n",
    "print(f'{travel_type} positive: {len(df_business_pos)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_business_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df_sample['token'].map(\" \".join).tolist()\n",
    "docs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"]=\"true\"\n",
    "\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "#from cuml.cluster import HDBSCAN\n",
    "#from cuml.manifold import UMAP\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "\n",
    "\n",
    "seed = 42\n",
    "\n",
    "# Step 1 - Extract embeddings\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Step 2 - Reduce dimensionality\n",
    "umap_model = UMAP(n_neighbors=30, #20,#15,\n",
    "                  n_components=2,\n",
    "                  min_dist=0.05,\n",
    "                  metric='cosine',\n",
    "                  random_state=seed)\n",
    "\n",
    "# Step 3 - Cluster reduced embeddings\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=300,\n",
    "                        metric='euclidean',\n",
    "                        cluster_selection_method='eom',\n",
    "                        prediction_data=True)\n",
    "\n",
    "# Step 4 - Tokenize topics\n",
    "#def tokenizer_split(text):\n",
    "#    return text.split(',')\n",
    "\n",
    "vectorizer_model = CountVectorizer(ngram_range=(2, 3), stop_words=\"english\")\n",
    "\n",
    "# Step 5 - Create topic representation\n",
    "ctfidf_model = ClassTfidfTransformer()\n",
    "\n",
    "# Step 6 - (Optional) Fine-tune topic representations with\n",
    "# a `bertopic.representation` model\n",
    "representation_model = KeyBERTInspired(top_n_words=100)\n",
    "\n",
    "# All steps together\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,          # Step 1 - Extract embeddings\n",
    "    umap_model=umap_model,                    # Step 2 - Reduce dimensionality\n",
    "    hdbscan_model=hdbscan_model,              # Step 3 - Cluster reduced embeddings\n",
    "    vectorizer_model=vectorizer_model,        # Step 4 - Tokenize topics\n",
    "    ctfidf_model=ctfidf_model,                # Step 5 - Extract topic words\n",
    "    representation_model=representation_model, # Step 6 - (Optional) Fine-tune topic represenations\n",
    "    top_n_words=100,\n",
    "    #min_topic_size=100,\n",
    ")\n",
    "\n",
    "topics, probs = topic_model.fit_transform(docs)\n",
    "#topic_model.fit_transform(docs)\n",
    "#topic_model.reduce_topics(docs, nr_topics='auto') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.save(f\"./output/model_{travel_type.replace('/','-')}_pos_{version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "topic_model.visualize_topics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "topic_model.visualize_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 濾掉 topic==-1 的 documents\n",
    "valid_docs_index = [idx for idx, topic in enumerate(topics) if topic != -1]\n",
    "filtered_docs = [docs[idx] for idx in valid_docs_index]\n",
    "filtered_topics = [topics[idx] for idx in valid_docs_index]\n",
    "\n",
    "topic_model.visualize_documents(docs=filtered_docs, topics=filtered_topics, embeddings=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "topic_model.visualize_barchart(topics=list(set(topics))[:10], n_words=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_heatmap()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "reduced_topics, reduced_probs = topic_model.reduce_topics(docs, nr_topics='auto')\n",
    "#topic_model.reduce_topics(docs, nr_topics=12) #auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "topic_model.visualize_topics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([topic_model.get_topic(i) for i in range(len(topic_model.topic_sizes_)-1)]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topickeywords = pd.DataFrame([topic_model.get_topic(i)\n",
    "                                 for i in range(len(topic_model.topic_sizes_)-1)]).transpose()\n",
    "\n",
    "df_topickeywords.to_pickle(f\"./output/df_keywords_{travel_type.replace('/','-')}_pos_{version}.pkl\")\n",
    "df_topickeywords.to_csv(f\"./output/keywords_{travel_type.replace('/','-')}_pos_{version}.csv\",\n",
    "                        encoding='utf-8', index=False)\n",
    "\n",
    "df_topickeywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(topic_model.topic_sizes_)-1):\n",
    "    print(f'### Topic {i}:')\n",
    "    print(f'```python!')\n",
    "    print(f'{[item[0] for item in topic_model.get_topic(i)[:30]]}')\n",
    "    print(f'```')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
