{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset Pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_pickle('./dataset/State_Hotel_reviews_v1_240415.01.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version='240418.01'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Friend/Couple Positive (4 or 5 顆星)\n",
    "\n",
    "Cleanliness, Room, Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleanliness, Room, Location\n",
    "travel_type = \"Friend/Couple\" #\"Family\", \"Business/Solo\"\n",
    "df_friend_pos = df[df['Travel Type Label'].str.contains(travel_type, na=False, regex=False)]\n",
    "df_friend_pos = df_friend_pos [#df_friend_pos [\"Overall Rating\"].isin([4, 5]) &\n",
    "           #df_friend_pos [\"Value\"].isin([4, 5]) &\n",
    "           df_friend_pos [\"Location\"].isin([4, 5]) &\n",
    "           df_friend_pos [\"Rooms\"].isin([4, 5]) &\n",
    "           #df_friend_pos [\"Service\"].isin([4, 5]) &\n",
    "           #df_friend_pos [\"Sleep Quality\"].isin([4, 5]) &\n",
    "           df_friend_pos [\"Cleanliness\"].isin([4, 5])]\n",
    "\n",
    "print(f'{travel_type} positive: {len(df_friend_pos)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_friend_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs = df['text'].tolist()\n",
    "docs = df_sample['token'].map(\" \".join).tolist()\n",
    "docs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"]=\"true\"\n",
    "\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "#from cuml.cluster import HDBSCAN\n",
    "#from cuml.manifold import UMAP\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "\n",
    "\n",
    "seed = 42\n",
    "\n",
    "# Step 1 - Extract embeddings\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Step 2 - Reduce dimensionality\n",
    "umap_model = UMAP(n_neighbors=30, #20,#15,\n",
    "                  n_components=2,\n",
    "                  min_dist=0.05,\n",
    "                  metric='cosine',\n",
    "                  random_state=seed)\n",
    "\n",
    "# Step 3 - Cluster reduced embeddings\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=300,\n",
    "                        metric='euclidean',\n",
    "                        cluster_selection_method='eom',\n",
    "                        prediction_data=True)\n",
    "\n",
    "# Step 4 - Tokenize topics\n",
    "#def tokenizer_split(text):\n",
    "#    return text.split(',')\n",
    "\n",
    "vectorizer_model = CountVectorizer(ngram_range=(2, 3), stop_words=\"english\")\n",
    "\n",
    "# Step 5 - Create topic representation\n",
    "ctfidf_model = ClassTfidfTransformer()\n",
    "\n",
    "# Step 6 - (Optional) Fine-tune topic representations with\n",
    "# a `bertopic.representation` model\n",
    "representation_model = KeyBERTInspired(top_n_words=100)\n",
    "\n",
    "# All steps together\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,          # Step 1 - Extract embeddings\n",
    "    umap_model=umap_model,                    # Step 2 - Reduce dimensionality\n",
    "    hdbscan_model=hdbscan_model,              # Step 3 - Cluster reduced embeddings\n",
    "    vectorizer_model=vectorizer_model,        # Step 4 - Tokenize topics\n",
    "    ctfidf_model=ctfidf_model,                # Step 5 - Extract topic words\n",
    "    representation_model=representation_model, # Step 6 - (Optional) Fine-tune topic represenations\n",
    "    top_n_words=100,\n",
    "    #min_topic_size=100,\n",
    ")\n",
    "\n",
    "topics, probs = topic_model.fit_transform(docs)\n",
    "#topic_model.fit_transform(docs)\n",
    "#topic_model.reduce_topics(docs, nr_topics='auto') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.save(f\"./output/model_{travel_type.replace('/','-')}_pos_{version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.reduce_topics(docs=docs, nr_topics='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.reduce_topics(docs=docs, nr_topics=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "topic_model.visualize_topics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "topic_model.visualize_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "topic_model.visualize_barchart(topics=list(set(topics))[:10], n_words=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.save(f\"./output/model_{travel_type.replace('/','-')}_pos_{version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topickeywords = pd.DataFrame([topic_model.get_topic(i)\n",
    "                                 for i in range(len(topic_model.topic_sizes_)-1)]).transpose()\n",
    "\n",
    "df_topickeywords.to_pickle(f\"./output/df_keywords_{travel_type.replace('/','-')}_pos_{version}.pkl\")\n",
    "df_topickeywords.to_csv(f\"./output/keywords_{travel_type.replace('/','-')}_pos_{version}.csv\",\n",
    "                        encoding='utf-8', index=False)\n",
    "\n",
    "df_topickeywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(topic_model.topic_sizes_)-1):\n",
    "    print(f'### Topic {i}:')\n",
    "    print(f'```python!')\n",
    "    print(f'{[item[0] for item in topic_model.get_topic(i)]}')\n",
    "    print(f'```')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
